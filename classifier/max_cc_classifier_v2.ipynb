{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/deeplearn/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! Files detected in test data directory!\n",
      "Moving files back to training data directory...\n",
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#============================================================================================\n",
    "# Classifier Training\n",
    "# Author: Gerald M (Changed by Maksim L)\n",
    "#\n",
    "# This script uses a convolution neural network classifier, to train for cells and non-cell\n",
    "# objects.\n",
    "#============================================================================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import datetime\n",
    "from multiprocessing import cpu_count\n",
    "import skimage.exposure\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import RMSprop\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['MKL_NUM_THREADS'] = str(cpu_count())\n",
    "os.environ['GOTO_NUM_THREADS'] = str(cpu_count())\n",
    "os.environ['OMP_NUM_THREADS'] = str(cpu_count())\n",
    "os.environ['openmp'] = 'True'\n",
    "\n",
    "config = tf.ConfigProto(device_count={\"GPU\" : 1, \"CPU\" : cpu_count()})\n",
    "keras.backend.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "\n",
    "def AHE(img):\n",
    "    img_adapteq = skimage.exposure.equalize_adapthist(img, clip_limit=0.03)\n",
    "    return img_adapteq\n",
    "\n",
    "#=============================================================================================\n",
    "# Precheck on directory structure\n",
    "#=============================================================================================\n",
    "\n",
    "if os.listdir('8-bit_lowfat/test_data/cell/'):\n",
    "    print 'Warning! Files detected in test data directory!'\n",
    "    print 'Moving files back to training data directory...'\n",
    "\n",
    "    for f in os.listdir('8-bit_lowfat/test_data/cell/'):\n",
    "        shutil.move('8-bit_lowfat/test_data/cell/'+f,'8-bit_lowfat/training_data/cell/'+f)\n",
    "\n",
    "    for f in os.listdir('8-bit_lowfat/test_data/nocell/'):\n",
    "        shutil.move('8-bit_lowfat/test_data/nocell/'+f,'8-bit_lowfat/training_data/nocell/'+f)\n",
    "        \n",
    "    print 'Done!\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Convolution Neural Network...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 78, 78, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 39, 39, 8)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 39, 39, 8)         32        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 37, 37, 16)        1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 18, 18, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 18, 18, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 6, 6, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 3, 3, 32)          128       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               36992     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 52,609\n",
      "Trainable params: 52,433\n",
      "Non-trainable params: 176\n",
      "_________________________________________________________________\n",
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#=============================================================================================\n",
    "# Construction of Convolution Neural Network\n",
    "#=============================================================================================\n",
    "\n",
    "print \"Constructing Convolution Neural Network...\"\n",
    "\n",
    "# Create object of sequential class\n",
    "classifier = Sequential()\n",
    "\n",
    "# Convolution operation\n",
    "# Number of filters\n",
    "# Shape of each filter\n",
    "# Input shape in first two, input type in third, 3 = RGB\n",
    "# activation function, relu = rectifier function\n",
    "classifier.add(Conv2D(8, (3, 3), input_shape = (80, 80, 1), activation = 'relu'))\n",
    "\n",
    "# Pooling operation\n",
    "# Pooling reduces size of images in order to reduce number of nodes\n",
    "# 2x2 matrix\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Batch normalization\n",
    "# Normalize the activations of the previous layer at each batch\n",
    "# i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "classifier.add(Conv2D(16, (3, 3), input_shape = (80, 80, 1), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (80, 80, 1), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (80, 80, 1), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "# Prevent nodes from generating same classifiers\n",
    "classifier.add(Dropout(0.5))\n",
    "\n",
    "# Flattening operation\n",
    "# Convert pooled images into vectors\n",
    "\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Connect the set of nodes which input to connected layers\n",
    "# Units is number of nodes\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "\n",
    "# Prevent nodes from generating same classifiers\n",
    "classifier.add(Dropout(0.5))\n",
    "\n",
    "# Initialise output layer\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid')) #softmax\n",
    "\n",
    "# Compile\n",
    "# Optimizer is stochastic gradient descent\n",
    "# Loss is loss function\n",
    "# Metric is performance metric\n",
    "optimizer = RMSprop(lr=1e-4)\n",
    "classifier.compile(optimizer = optimizer, loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "classifier.summary()\n",
    "\n",
    "print \"Done!\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting all training data into 70% training and 30% test data directories...\n",
      "Found 20 images belonging to 2 classes.\n",
      "Found 178 images belonging to 2 classes.\n",
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#=============================================================================================\n",
    "# Preparing training and test data\n",
    "#=============================================================================================\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "print \"Splitting all training data into 70% training and 30% test data directories...\"\n",
    "\n",
    "cell_data = os.listdir('8-bit_lowfat/training_data/cell/')\n",
    "nocell_data = os.listdir('8-bit_lowfat/training_data/nocell/')\n",
    "\n",
    "validation_cell_data = random.sample(cell_data, int(0.3*len(cell_data)))\n",
    "validation_nocell_data = random.sample(nocell_data, int(0.3*len(nocell_data)))\n",
    "\n",
    "for f in validation_cell_data:\n",
    "    shutil.move('8-bit_lowfat/training_data/cell/'+f,'8-bit_lowfat/test_data/cell/'+f)\n",
    "\n",
    "for f in validation_nocell_data:\n",
    "    shutil.move('8-bit_lowfat/training_data/nocell/'+f,'8-bit_lowfat/test_data/nocell/'+f)\n",
    "\n",
    "# training data\n",
    "#train_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function=p.keras_preprocess_func())\n",
    "#training_data = train_datagen.flow_from_directory('8-bit_lowfat/training_data', target_size = (80, 80), batch_size = 32, class_mode = 'binary', color_mode = 'grayscale')\n",
    "#training_data = train_datagen.flow_from_directory('8-bit_lowfat/training_data', target_size = (80, 80), batch_size = 32, class_mode = 'binary', color_mode = 'grayscale', save_to_dir='preview', save_prefix='cell', save_format='jpeg')\n",
    "# validation data\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_data = test_datagen.flow_from_directory('8-bit_lowfat/test_data', target_size = (80, 80), batch_size = 32, class_mode = 'binary', color_mode = 'grayscale')\n",
    "\n",
    "print \"Done!\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operations: 10\n",
      "\t0: RotateRange (max_right_rotation=15.0 max_left_rotation=-15.0 probability=0.7 )\n",
      "\t1: Zoom (min_factor=1.1 max_factor=1.5 probability=0.2 )\n",
      "\t2: Skew (magnitude=1 skew_type=TILT_TOP_BOTTOM probability=0.2 )\n",
      "\t3: Skew (magnitude=1 skew_type=TILT_LEFT_RIGHT probability=0.2 )\n",
      "\t4: Shear (max_shear_right=10 max_shear_left=10 probability=0.3 )\n",
      "\t5: RotateStandard (max_right_rotation=30.0 max_left_rotation=-30.0 expand=False probability=0.3 )\n",
      "\t6: Rotate 90 (rotation=90 probability=0.2 )\n",
      "\t7: Rotate 180 (rotation=180 probability=0.2 )\n",
      "\t8: Distort (randomise_magnitude=True magnitude=2 grid_height=5 probability=0.3 grid_width=5 )\n",
      "\t9: RandomBrightness (min_factor=0.8 max_factor=1.2 probability=0.5 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Found 20 images belonging to 2 classes.\n",
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import Augmentor\n",
    "#import torchvision\n",
    "\n",
    "#keras.utils.np_utils.to_categorical(training_data,2)\n",
    "#p = Augmentor.Pipeline('8-bit_lowfat/training_data/', output_directory='../../output')\n",
    "p = Augmentor.Pipeline()\n",
    "p.rotate(probability=0.7, max_left_rotation=15, max_right_rotation=15)\n",
    "p.zoom(probability=0.2, min_factor=1.1, max_factor=1.5)\n",
    "p.skew_top_bottom(probability=0.2, magnitude=1)\n",
    "p.skew_left_right(probability=0.2, magnitude=1)\n",
    "p.shear(probability=0.3, max_shear_left=10, max_shear_right=10)\n",
    "p.rotate_without_crop(probability=0.3, max_left_rotation=30, max_right_rotation=30, expand=False)\n",
    "p.rotate90(probability=0.2)\n",
    "p.rotate180(probability=0.2)\n",
    "p.random_distortion(probability=0.3, grid_width=5, grid_height=5, magnitude=2)\n",
    "p.random_brightness(probability=0.5, min_factor=0.8, max_factor=1.2)\n",
    "\n",
    "\n",
    "p.status()\n",
    "\n",
    "\n",
    "#batch_size = 32\n",
    "#g = p.keras_generator(batch_size=batch_size)\n",
    "\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=p.keras_preprocess_func())\n",
    "training_data = train_datagen.flow_from_directory('8-bit_lowfat/output', target_size = (80, 80), batch_size = 32, class_mode = 'binary', color_mode = 'grayscale', save_format='tif')\n",
    "\n",
    "print \"Done!\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.04705882]\n",
      "  [0.05490196]\n",
      "  [0.03529412]\n",
      "  ...\n",
      "  [0.05882353]\n",
      "  [0.03529412]\n",
      "  [0.05490196]]\n",
      "\n",
      " [[0.02352941]\n",
      "  [0.00784314]\n",
      "  [0.03137255]\n",
      "  ...\n",
      "  [0.05882353]\n",
      "  [0.10196079]\n",
      "  [0.09803922]]\n",
      "\n",
      " [[0.04313726]\n",
      "  [0.00392157]\n",
      "  [0.01568628]\n",
      "  ...\n",
      "  [0.14901961]\n",
      "  [0.08627451]\n",
      "  [0.17254902]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  ...\n",
      "  [0.01176471]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.01176471]\n",
      "  [0.02352941]\n",
      "  ...\n",
      "  [0.02745098]\n",
      "  [0.01568628]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.03921569]\n",
      "  [0.05098039]\n",
      "  ...\n",
      "  [0.03921569]\n",
      "  [0.04313726]\n",
      "  [0.        ]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = next(g, None)\n",
    "print(images[0])\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model to data...\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "Cannot handle this data type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ada3aa3aa6f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# steps_per_epoch is number of images in training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Done!\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/deeplearn/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/deeplearn/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1313\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/deeplearn/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/deeplearn/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2192\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2193\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2194\u001b[0;31m                     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2196\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/deeplearn/lib/python2.7/site-packages/keras/utils/data_utils.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/deeplearn/lib/python2.7/site-packages/six.pyc\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: Cannot handle this data type"
     ]
    }
   ],
   "source": [
    "#=============================================================================================\n",
    "# Fitting model to data\n",
    "#=============================================================================================\n",
    "\n",
    "print \"Fitting model to data...\"\n",
    "\n",
    "# Find number of epoch and validation steps\n",
    "steps_epoch = len([filename for filename in os.listdir('8-bit_lowfat/training_data/cell') if filename.endswith(\".tif\")]) + len([filename for filename in os.listdir('8-bit_lowfat/training_data/nocell') if filename.endswith(\".tif\")])\n",
    "steps_valid = len([filename for filename in os.listdir('8-bit_lowfat/test_data/cell') if filename.endswith(\".tif\")]) + len([filename for filename in os.listdir('8-bit_lowfat/test_data/nocell') if filename.endswith(\".tif\")])\n",
    "\n",
    "#steps_epoch=len(p.augmentor_images)/batch_size\n",
    "\n",
    "# Checkpoint to only save the best model, metric = val_acc\n",
    "filepath = \"cc_model_\"+datetime.datetime.today().strftime('%Y_%m_%d')+\".h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "earlystop = EarlyStopping(monitor='acc', patience=1,)\n",
    "callbacks_list = [checkpoint, earlystop]\n",
    "\n",
    "\n",
    "# steps_per_epoch is number of images in training set\n",
    "history = classifier.fit_generator(training_data, steps_per_epoch = steps_epoch, epochs = 30, callbacks=callbacks_list, validation_data = test_data, validation_steps = steps_valid, shuffle = True, verbose=1)\n",
    "\n",
    "print \"Done!\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================================================\n",
    "# Plotting accuracy\n",
    "#=============================================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "plt.savefig('Acc_starting.png')\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "plt.savefig('Loss_starting.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally evaluating model on the test data\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "'8-bit/training_data',\n",
    "target_size=(80, 80),\n",
    "batch_size=32,\n",
    "class_mode='binary',\n",
    "color_mode = 'grayscale',\n",
    "shuffle=False)\n",
    "test_loss, test_acc = classifier.evaluate_generator(test_generator, steps=200)\n",
    "print('test acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating confusion matrix\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "#predictions = classifier.predict_generator(test_generator)\n",
    "#matrix = metrics.confusion_matrix(test_generator, predictions)\n",
    "#print(matrix)\n",
    "\n",
    "#Confution Matrix and Classification Report\n",
    "Y_pred = classifier.predict_generator(test_generator)\n",
    "Y_pred[Y_pred > 0.5] = 1\n",
    "Y_pred[Y_pred < 0.5] = 0\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(test_generator.classes, Y_pred))\n",
    "print('Classification Report')\n",
    "target_names = ['Cell','Nocell']\n",
    "print(classification_report(test_generator.classes, Y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
