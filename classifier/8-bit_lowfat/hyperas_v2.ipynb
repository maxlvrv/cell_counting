{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/deeplearn/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! Files detected in test data directory!\n",
      "Moving files back to training data directory...\n",
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import skimage as ski\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "\n",
    "#=============================================================================================\n",
    "# Precheck on directory structure\n",
    "#=============================================================================================\n",
    "\n",
    "if os.listdir('test_data/cell/'):\n",
    "    print('Warning! Files detected in test data directory!')\n",
    "    print('Moving files back to training data directory...')\n",
    "\n",
    "    for f in os.listdir('test_data/cell/'):\n",
    "        shutil.move('test_data/cell/'+f,'training_data/cell/'+f)\n",
    "\n",
    "    for f in os.listdir('test_data/nocell/'):\n",
    "        shutil.move('test_data/nocell/'+f,'training_data/nocell/'+f)\n",
    "        \n",
    "    print('Done!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cell_data = os.listdir('training_data/cell/')\n",
    "nocell_data = os.listdir('training_data/nocell/')\n",
    "\n",
    "validation_cell_data = random.sample(cell_data, int(0.3*len(cell_data)))\n",
    "validation_nocell_data = random.sample(nocell_data, int(0.3*len(nocell_data)))\n",
    "\n",
    "for f in validation_cell_data:\n",
    "    shutil.move('training_data/cell/'+f,'test_data/cell/'+f)\n",
    "\n",
    "for f in validation_nocell_data:\n",
    "    shutil.move('training_data/nocell/'+f,'test_data/nocell/'+f)\n",
    "\n",
    "print('Done!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "from __future__ import print_function\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import random\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import shutil\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import skimage as ski\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.image as mpimg\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'model_choice': hp.choice('model_choice', ['one', 'two', 'three']),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
      "        'Dropout_2': hp.uniform('Dropout_2', 0, 1),\n",
      "        'Dropout_3': hp.uniform('Dropout_3', 0, 1),\n",
      "        'Dropout_4': hp.uniform('Dropout_4', 0, 1),\n",
      "        'Dense': hp.choice('Dense', [128, 256, 512]),\n",
      "        'Activation': hp.choice('Activation', ['relu', 'sigmoid']),\n",
      "        'Dropout_5': hp.uniform('Dropout_5', 0, 1),\n",
      "        'Dense_1': hp.choice('Dense_1', [256, 512, 1024]),\n",
      "        'Activation_1': hp.choice('Activation_1', ['relu', 'sigmoid']),\n",
      "        'Dropout_6': hp.uniform('Dropout_6', 0, 1),\n",
      "        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'sgd']),\n",
      "        'batch_size': hp.choice('batch_size', [64, 128]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: \"\"\"\n",
      "   3: Data providing function:\n",
      "   4: \n",
      "   5: This function is separated from create_model() so that hyperopt\n",
      "   6: won't reload data for each evaluation run.\n",
      "   7: \"\"\"  \n",
      "   8: cell_data = os.listdir('training_data/cell/')\n",
      "   9: nocell_data = os.listdir('training_data/nocell/')\n",
      "  10: \n",
      "  11: # Training data\n",
      "  12: path = 'training_data/cell/'\n",
      "  13: x_train = []\n",
      "  14: y_train = []\n",
      "  15: for f in cell_data:\n",
      "  16:     im = mpimg.imread(str(path)+f)\n",
      "  17:     x_train.append(im)\n",
      "  18:     y_train.append(1)\n",
      "  19: path = 'training_data/nocell/'\n",
      "  20: for f in nocell_data:\n",
      "  21:     im = mpimg.imread(str(path)+f)\n",
      "  22:     x_train.append(im)\n",
      "  23:     y_train.append(0)\n",
      "  24: \n",
      "  25: cell_test_data = os.listdir('test_data/cell/')\n",
      "  26: nocell_test_data = os.listdir('test_data/nocell/')\n",
      "  27: \n",
      "  28: # Validation data\n",
      "  29: path = 'test_data/cell/'\n",
      "  30: x_test = []\n",
      "  31: y_test = []\n",
      "  32: for f in cell_test_data:\n",
      "  33:     im = mpimg.imread(str(path)+f)\n",
      "  34:     x_test.append(im)\n",
      "  35:     y_test.append(1)\n",
      "  36: path = 'test_data/nocell/'\n",
      "  37: for f in nocell_test_data:\n",
      "  38:     im = mpimg.imread(str(path)+f)\n",
      "  39:     x_test.append(im)\n",
      "  40:     y_test.append(0)\n",
      "  41:     \n",
      "  42: \n",
      "  43: x_train = np.asarray(x_train, dtype=np.float32)\n",
      "  44: y_train = np.asarray(y_train, dtype=np.float32)\n",
      "  45: x_test = np.asarray(x_test, dtype=np.float32)\n",
      "  46: y_test = np.asarray(y_test, dtype=np.float32)\n",
      "  47: \n",
      "  48: x_train = x_train.reshape(x_train.shape[0], 80, 80, 1)\n",
      "  49: x_test = x_test.reshape(x_test.shape[0], 80, 80, 1)\n",
      "  50: x_train = x_train.astype('float32')\n",
      "  51: x_test = x_test.astype('float32')\n",
      "  52: x_train /= 255\n",
      "  53: x_test /= 255\n",
      "  54: nb_classes = 2\n",
      "  55: y_train = np_utils.to_categorical(y_train, nb_classes)\n",
      "  56: y_test = np_utils.to_categorical(y_test, nb_classes)\n",
      "  57: \n",
      "  58: \n",
      "  59: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \"\"\"\n",
      "   4:     Model providing function:\n",
      "   5: \n",
      "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "  10:     The last one is optional, though recommended, namely:\n",
      "  11:         - model: specify the model just created so that we can later use it again.\n",
      "  12:     \"\"\"\n",
      "  13:     model = Sequential()\n",
      "  14:     model.add(Conv2D(8, (3, 3), input_shape = (80, 80, 1), activation = 'relu'))\n",
      "  15:     model.add(MaxPooling2D(pool_size = (2, 2)))\n",
      "  16:     \n",
      "  17:     model_choice = space['model_choice']\n",
      "  18:     if model_choice == 'one':\n",
      "  19:         model.add(Conv2D(16, kernel_size=3, activation='relu',padding='same', input_shape=(1,28,28), data_format='channels_first'))\n",
      "  20:         model.add(Conv2D(16, kernel_size=3, activation='relu',padding='same'))\n",
      "  21:         model.add(MaxPooling2D(pool_size=2,strides=2))\n",
      "  22:         model.add(Dropout(space['Dropout']))\n",
      "  23:         model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
      "  24:         model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
      "  25:         model.add(BatchNormalization())\n",
      "  26:         model.add(MaxPooling2D(pool_size=2,strides=2))\n",
      "  27:         model.add(Dropout(space['Dropout_1']))\n",
      "  28:     elif model_choice == 'two':\n",
      "  29:         model.add(Conv2D(32, kernel_size=3, activation='relu',padding='same', input_shape=(1,28,28), data_format='channels_first'))\n",
      "  30:         model.add(Conv2D(32, kernel_size=3, activation='relu',padding='same'))\n",
      "  31:         model.add(MaxPooling2D(pool_size=2,strides=2))\n",
      "  32:         model.add(Dropout(space['Dropout_2']))\n",
      "  33:         model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
      "  34:         model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
      "  35:         model.add(BatchNormalization())\n",
      "  36:         model.add(MaxPooling2D(pool_size=2,strides=2))\n",
      "  37:         model.add(Dropout(space['Dropout_3']))\n",
      "  38: # Below is Gerald's original model\n",
      "  39:     elif model_choice == 'three':\n",
      "  40:         model.add(Conv2D(16, (3, 3), input_shape = (80, 80, 1), activation = 'relu'))\n",
      "  41:         model.add(MaxPooling2D(pool_size = (2, 2)))\n",
      "  42:         model.add(BatchNormalization())\n",
      "  43:         model.add(Conv2D(32, (3, 3), input_shape = (80, 80, 1), activation = 'relu'))\n",
      "  44:         model.add(MaxPooling2D(pool_size = (2, 2)))\n",
      "  45:         model.add(BatchNormalization())\n",
      "  46:         model.add(Conv2D(32, (3, 3), input_shape = (80, 80, 1), activation = 'relu'))\n",
      "  47:         model.add(MaxPooling2D(pool_size = (2, 2)))\n",
      "  48:         model.add(BatchNormalization())\n",
      "  49:         model.add(Dropout(space['Dropout_4']))\n",
      "  50:         model.add(Flatten())\n",
      "  51:         model.add(Dense(space['Dense']))\n",
      "  52:         model.add(Activation(space['Activation']))\n",
      "  53:         model.add(Dropout(space['Dropout_5']))\n",
      "  54:     \n",
      "  55:     model.add(Dense(space['Dense_1']))\n",
      "  56:     model.add(Activation(space['Activation_1']))\n",
      "  57:     model.add(Dropout(space['Dropout_6']))\n",
      "  58: \n",
      "  59:     model.add(Flatten())\n",
      "  60:     model.add(Dense(2, activation='softmax'))\n",
      "  61:     \n",
      "  62:     model.summary()\n",
      "  63: \n",
      "  64:     model.compile(loss='binary_crossentropy', metrics=['accuracy'],\n",
      "  65:                   optimizer=space['optimizer'])\n",
      "  66: \n",
      "  67:     result = model.fit(x_train, y_train,\n",
      "  68:               batch_size=space['batch_size'],\n",
      "  69:               epochs=2,\n",
      "  70:               verbose=2,\n",
      "  71:               validation_split=0.1)\n",
      "  72:     #get the highest validation accuracy of the training epochs\n",
      "  73:     validation_acc = np.amax(result.history['val_acc']) \n",
      "  74:     print('Best validation acc of epoch:', validation_acc)\n",
      "  75:     return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
      "  76: \n",
      "  0%|          | 0/5 [00:00<?, ?it/s, best loss: ?]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'Conv2D' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d56374299d86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m                                           \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                                           \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                                           notebook_name='hyperas')\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evalutation of best performing model:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/deeplearn/lib/python2.7/site-packages/hyperas/optim.pyc\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space, keep_temp)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                                      \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                                      keep_temp=keep_temp)\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/deeplearn/lib/python2.7/site-packages/hyperas/optim.pyc\u001b[0m in \u001b[0;36mbase_minimizer\u001b[0;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack, keep_temp)\u001b[0m\n\u001b[1;32m    137\u001b[0m              \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m              \u001b[0mrstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m              return_argmin=True),\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mget_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     )\n",
      "\u001b[0;32m/opt/anaconda/envs/deeplearn/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/deeplearn/lib/python2.7/site-packages/hyperopt/base.pyc\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/deeplearn/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/deeplearn/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/deeplearn/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/deeplearn/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/deeplearn/lib/python2.7/site-packages/hyperopt/base.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtemp_model.py\u001b[0m in \u001b[0;36mkeras_fmin_fnct\u001b[0;34m(space)\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'Conv2D' is not defined"
     ]
    }
   ],
   "source": [
    "def data():\n",
    "    \"\"\"\n",
    "    Data providing function:\n",
    "\n",
    "    This function is separated from create_model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    \"\"\"  \n",
    "    cell_data = os.listdir('training_data/cell/')\n",
    "    nocell_data = os.listdir('training_data/nocell/')\n",
    "\n",
    "    # Training data\n",
    "    path = 'training_data/cell/'\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for f in cell_data:\n",
    "        im = mpimg.imread(str(path)+f)\n",
    "        x_train.append(im)\n",
    "        y_train.append(1)\n",
    "    path = 'training_data/nocell/'\n",
    "    for f in nocell_data:\n",
    "        im = mpimg.imread(str(path)+f)\n",
    "        x_train.append(im)\n",
    "        y_train.append(0)\n",
    "\n",
    "    cell_test_data = os.listdir('test_data/cell/')\n",
    "    nocell_test_data = os.listdir('test_data/nocell/')\n",
    "\n",
    "    # Validation data\n",
    "    path = 'test_data/cell/'\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    for f in cell_test_data:\n",
    "        im = mpimg.imread(str(path)+f)\n",
    "        x_test.append(im)\n",
    "        y_test.append(1)\n",
    "    path = 'test_data/nocell/'\n",
    "    for f in nocell_test_data:\n",
    "        im = mpimg.imread(str(path)+f)\n",
    "        x_test.append(im)\n",
    "        y_test.append(0)\n",
    "        \n",
    "    \n",
    "    x_train = np.asarray(x_train, dtype=np.float32)\n",
    "    y_train = np.asarray(y_train, dtype=np.float32)\n",
    "    x_test = np.asarray(x_test, dtype=np.float32)\n",
    "    y_test = np.asarray(y_test, dtype=np.float32)\n",
    "\n",
    "    x_train = x_train.reshape(x_train.shape[0], 80, 80, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 80, 80, 1)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    nb_classes = 2\n",
    "    y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "    y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    model_choice = {{choice(['one', 'two', 'three'])}}\n",
    "    if model_choice == 'one':\n",
    "        model.add(Conv2D(16, kernel_size=3, activation='relu',padding='same', input_shape=(1,28,28), data_format='channels_first'))\n",
    "        model.add(Conv2D(16, kernel_size=3, activation='relu',padding='same'))\n",
    "        model.add(MaxPooling2D(pool_size=2,strides=2))\n",
    "        model.add(Dropout({{uniform(0, 1)}}))\n",
    "        model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "        model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=2,strides=2))\n",
    "        model.add(Dropout({{uniform(0, 1)}}))\n",
    "    elif model_choice == 'two':\n",
    "        model.add(Conv2D(32, kernel_size=3, activation='relu',padding='same', input_shape=(1,28,28), data_format='channels_first'))\n",
    "        model.add(Conv2D(32, kernel_size=3, activation='relu',padding='same'))\n",
    "        model.add(MaxPooling2D(pool_size=2,strides=2))\n",
    "        model.add(Dropout({{uniform(0, 1)}}))\n",
    "        model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "        model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=2,strides=2))\n",
    "        model.add(Dropout({{uniform(0, 1)}}))\n",
    "# Below is Gerald's original model\n",
    "    elif model_choice == 'three':\n",
    "        model.add(Conv2D(8, (3, 3), input_shape = (80, 80, 1), activation = 'relu'))\n",
    "        model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "        model.add(Conv2D(16, (3, 3), input_shape = (80, 80, 1), activation = 'relu'))\n",
    "        model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(32, (3, 3), input_shape = (80, 80, 1), activation = 'relu'))\n",
    "        model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(32, (3, 3), input_shape = (80, 80, 1), activation = 'relu'))\n",
    "        model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout({{uniform(0, 1)}}))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense({{choice([128, 256, 512])}}))\n",
    "        model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
    "        model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    model.add(Dense({{choice([256, 512, 1024])}}))\n",
    "    model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy'],\n",
    "                  optimizer={{choice(['rmsprop', 'adam', 'sgd'])}})\n",
    "\n",
    "    result = model.fit(x_train, y_train,\n",
    "              batch_size={{choice([64, 128])}},\n",
    "              epochs=2,\n",
    "              verbose=2,\n",
    "              validation_split=0.1)\n",
    "    #get the highest validation accuracy of the training epochs\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    best_run, best_model = optim.minimize(model=create_model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=5,\n",
    "                                          trials=Trials(),\n",
    "                                          notebook_name='hyperas')\n",
    "    X_train, Y_train, X_test, Y_test = data()\n",
    "    print(\"Evalutation of best performing model:\")\n",
    "    print(best_model.evaluate(X_test, Y_test))\n",
    "    print(\"Best performing model chosen hyper-parameters:\")\n",
    "    print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell_data = os.listdir('training_data/cell/')\n",
    "nocell_data = os.listdir('training_data/nocell/')\n",
    "\n",
    "# Training data\n",
    "path = 'training_data/cell/'\n",
    "x_train = []\n",
    "y_train = []\n",
    "for f in cell_data:\n",
    "    im = mpimg.imread(str(path)+f)\n",
    "    x_train.append(im)\n",
    "    y_train.append(1)\n",
    "path = 'training_data/nocell/'\n",
    "for f in nocell_data:\n",
    "    im = mpimg.imread(str(path)+f)\n",
    "    x_train.append(im)\n",
    "    y_train.append(0)\n",
    "    \n",
    "cell_test_data = os.listdir('test_data/cell/')\n",
    "nocell_test_data = os.listdir('test_data/nocell/')\n",
    "    \n",
    "# Validation data\n",
    "path = 'test_data/cell/'\n",
    "x_test = []\n",
    "y_test = []\n",
    "for f in cell_test_data:\n",
    "    im = mpimg.imread(str(path)+f)\n",
    "    x_test.append(im)\n",
    "    y_test.append(1)\n",
    "path = 'test_data/nocell/'\n",
    "for f in nocell_test_data:\n",
    "    im = mpimg.imread(str(path)+f)\n",
    "    x_test.append(im)\n",
    "    y_test.append(0)\n",
    "    \n",
    "print(x_train, y_train, x_test, y_test) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
